### Regresja liniowa

Regresja liniowa to podstawowa metoda statystyczna stosowana w uczeniu maszynowym do modelowania relacji między jedną lub większą liczbą zmiennych niezależnych (predyktorów) a zmienną zależną (wynikiem). Głównym celem regresji liniowej jest znalezienie krzywej regresji, która najlepiej przewiduje zmienną zależną na podstawie zmiennych niezależnych.

#### Wzór ogólny
Dla prostego modelu regresji liniowej z jednym predyktorem to:

$$
\hat y = \beta_0 1 + \beta_1 x + \epsilon
$$

Gdzie:
- $ \hat **y $  to przewidywana wartość (zmienna zależna).
- $ x $  to zmienna niezależna (predyktor).
- $ \beta_0 $  to przecięcie y linii regresji.
- $ \beta_1 $  to nachylenie linii regresji, reprezentujące zmianę $ y $  dla zmiany $ x $  o jedną jednostkę.
- $ \epsilon $  jest błędem statystycznym, uwzględniającym zmienność w $ y $  niewyjaśnioną przez $ x $ .

W przypadku wielu zmiennych regresji liniowej, w których występuje wiele predyktorów, wzór rozszerza się do:

$$
y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \epsilon_i = \boldsymbol{x}_i^{\top} \boldsymbol{\beta} + \epsilon_i
$$

Gdzie:
- $ x_1, x_2, \ldots, x_n $  są zmiennymi niezależnymi.
- $ \beta_1, \beta_2, \ldots, \beta_n $  są współczynnikami odpowiadającymi każdemu predyktorowi.

Najogólniejszy zapis macierzowy:
$$
 \mathbf{y} = X {\boldsymbol{\beta }} + {\boldsymbol{\varepsilon }}
$$

Gdzie:
$$
 \mathbf{y} = { 
    \begin{pmatrix} y_{1} \\ y_{2} \\\vdots \\y_{n} \end{pmatrix} 
    },

\qquad X = {
    \begin{pmatrix} \mathbf{x}_{1}^{\top } \\ \mathbf{x} _{2}^{\top} \\ \vdots \\ \mathbf{x}_{n}^{\top} \end{pmatrix} 
    } = {
    \begin{pmatrix} 1 & x_{11} & \ldots & x_{1p} \\ 1 & x_{21} & \ldots & x_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n1} & \ldots & x_{np} \end{pmatrix}
    },

\qquad{ \boldsymbol{\beta}} = {
    \begin{pmatrix} \beta_{0} \\ \beta_{1} \\ \beta_{2} \\ \vdots \\ \beta_{p} \end{pmatrix}
    },
    
\qquad{ \boldsymbol{\varepsilon}} = {
    \begin{pmatrix} \varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n} \end{pmatrix}
    }
$$

Regresja liniowa, jako jedna z podstawowych metod analizy statystycznej, opiera się na kilku kluczowych założeniach, które są niezbędne do prawidłowego funkcjonowania modelu. Pierwszym z tych założeń jest liniowość, co oznacza, że związek między zmiennymi niezależnymi a zmienną zależną jest liniowy. Oznacza to, że zmiany w predyktorach powinny prowadzić do proporcjonalnych zmian w wyniku.

Kolejnym istotnym założeniem jest niezależność obserwacji. Każda obserwacja powinna być niezależna od pozostałych, co jest kluczowe dla uzyskania wiarygodnych wyników. 

Trzecim założeniem jest homoskedastyczność, które odnosi się do stałej wariancji reszt, czyli błędów modelu, na wszystkich poziomach zmiennych niezależnych. To zapewnia, że nie występują systematyczne różnice w błędach w różnych zakresach wartości zmiennych.

Ostatnim jest normalizacja reszt i błędów. Oczekuje się, że błędy modelu będą miały rozkład normalny, co jest istotne dla przeprowadzania testów statystycznych i oceny jakości modelu.

Dopasowanie modelu regresji liniowej zazwyczaj realizowane jest przy użyciu metod takich jak "najmniejsze kwadraty" (least squares - LS).

$$
L = \sum_{i=1}^{n} (y_i - (\boldsymbol{x}_i^{\top} \boldsymbol{\beta} + \epsilon_i))^2 
$$

Metoda ta minimalizuje sumę kwadratów różnic między wartościami obserwowanymi a przewidywanymi przez model, co pozwala na uzyskanie jak najlepszego dopasowania do danych.

Aby ocenić jakość dopasowanego modelu regresji liniowej, stosuje się różnorodne metryki. Najpopularniejszą jest $ R^2 $ (R - współczynnik korelacji Pearsona), która wskazuje, jak dobrze zmienne niezależne wyjaśniają zmienność zmiennej zależnej. Wartości $ R^2 $ mieszczą się w zakresie od 0 do 1, przy czym wyższe wartości oznaczają lepsze dopasowanie modelu do danych. 

Zaletami regresji liniowej jest łatwość do zinterpretowania, co umożliwia użytkownikom zrozumienie relacji między zmiennymi. Jest też wydajna obliczeniowo. To powoduje że jest odpowiednią do pracy z dużymi zbiorami danych.

Jednakże ma również swoje ograniczenia. Jest wrażliwa na wartości odstające, które mogą znacząco wpłynąć na wyniki modelu. Zakłada liniową zależność między predyktorami a zmienną odpowiedzi, co może być problematyczne w przypadku bardziej złożonych relacji nieliniowych.

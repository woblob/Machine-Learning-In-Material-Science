### Regresja LASSO

Regresja LASSO, co oznacza "operator najmniejszej bezwzględnej redukcji i wyboru" (Least Absolute Shrinkage and Selection Operator). Jest to technika regresji liniowej, która obejmuje regularyzację L1. Ta metoda nie tylko pomaga zapobiegać nadmiernemu dopasowaniu, ale także wykonuje selekcję zmiennych poprzez zmniejszenie niektórych współczynników do dokładnie zera, skutecznie wykluczając te zmienne z modelu.

#### Wzór ogólny
Wzór na krzywa regresji jest taki sam jak w przypadku regresji liniowej. Istotną różnicą jest sposób wyznaczania współczynników krzywej.

Funkcję kosztu dla regresji Lasso można wyrazić jako:

$$
L = \sum_{i=1}^{n} (y_i - (\boldsymbol{x}_i^{\top} \boldsymbol{\beta} + \epsilon_i))^2 + \lambda \sum_{j=1}^{n} |\beta_j|
$$

Gdzie:
- $ L $ jest całkowitą funkcją kosztu.
- $ y_i $ są rzeczywistymi wartościami zmiennej zależnej.
- $ \boldsymbol{x}_i^{\top} $ to transponowany wektor zmiennych niezależnych (cechy).
- $ \boldsymbol{\beta} $ to wektor współczynników modelu.
- $ \lambda $ to stopień regularyzacji, który kontroluje "człon regularyzacyjny" $( \lambda \sum_{j=1}^{n} |\beta_j|)$.

#### Główne cechy

**Regularyzacja**: Człon regularyzacyjny L1 ($ \lambda \sum_{j=1}^{n} |\beta_j| $) generuje "model rzadki" dla współczynników $ \boldsymbol{ \beta } $. Oznacza to, że mniej ważne cechy można całkowicie usunąć z modelu, co ułatwia interpretację.

**Kompromis między odchyleniem a wariancją**: Wprowadzenie regularyzacji pomaga zarządzać kompromisem między odchyleniem a wariancją. Chociaż może wprowadzać pewne odchylenie do przewidywań, może znacznie zmniejszyć wariancję, co prowadzi do lepszej wydajności w przypadku niewidzianych danych.


#### Zalety
Regresja Lasso charakteryzuje się prostotą i łatwością interpretacji. To prowadzi do tworzenia modeli, które są łatwiejsze do zrozumienia, zwłaszcza w przypadku zbiorów danych o dużej liczbie wymiarów. Dodatkowo, potrafi efektywnie obsługiwać sytuacje, w których zmienne niezależne są silnie skorelowane. W takich przypadkach model wybiera jedną zmienną z grupy skorelowanych predyktorów, co pozwala na uproszczenie modelu i eliminację problemu współliniowości.

#### Ograniczenia
Ograniczeniami są między innymi błąd tendencyjności danych. W sytuacjach silnej korelacji między zmiennymi, Lasso może preferować jedną zmienną kosztem innych istotnych predyktorów, co może prowadzić do utraty cennych informacji. Ponadto, metoda ta jest wrażliwa na wybór parametru regularyzacji $ \lambda $. Zbyt wysoka wartość tego parametru może prowadzić do niedopasowania modelu, podczas gdy zbyt niska wartość może nie wystarczająco rozwiązać problemu nadmiernego dopasowania.

#### Zastosowanie
Regresja Lasso jest szeroko stosowana w każdej dziedzinie, w której selekcja cech ma kluczowe znaczenie ze względu na dużą wymiarowość.

### Regresja grzbietowa

Regresja grzbietowa to technika regresji liniowej, która obejmuje regularyzację L2 w celu rozwiązania problemów współliniowości i nadmiernego dopasowania. Dodając człon kary do funkcji kosztu, regresja grzbietowa pomaga ustabilizować oszacowania współczynników, szczególnie gdy istnieją wysokie korelacje między predyktorami.

#### Wzóry ogólne:
Jawny wzór regresji grzbietowej:
$$
\boldsymbol{\hat \beta} = ( \boldsymbol{X}^T \boldsymbol{X} + \lambda \boldsymbol{I} )^{-1} \boldsymbol{X}^T \boldsymbol{y}
$$

gdzie:

- $ \boldsymbol{\hat \beta}  $ to przedstawia wektor szacowanych współczynników (lub wag) dla modelu regresji grzbietowej.
- $ \boldsymbol{X} $ to macierz cech zawierająca zmienne niezależne (predyktory) używane w regresji. Każdy wiersz odpowiada obserwacji, a każda kolumna odpowiada cesze.
- $ \boldsymbol{X}^T $ to transponowana macierz $ \boldsymbol{X} $. 
- $ \lambda $ to stopień regularyzacji. Kontroluje on ilość skurczu stosowanego do współczynników. Wyższa wartość λλ skutkuje większym skurczem, co pomaga zmniejszyć nadmierne dopasowanie poprzez karanie dużych wartości współczynników.
- $ \boldsymbol{I} $: To przedstawia macierz jednostkową, która jest macierzą kwadratową z jedynkami na przekątnej i zerami w innych miejscach. 
- $ \boldsymbol{y} $: To wektor odpowiedzi zawierający obserwacje zmiennej zależnej, którą próbujemy przewidzieć.


Funkcję kosztu dla regresji grzbietowej można wyrazić jako:

$$
L = \sum_{i=1}^{n} (y_i - (\boldsymbol{x}_i^{\top} \boldsymbol{\beta} + \epsilon_i))^2 + \lambda \sum_{j=1}^{n} \beta_j^2
$$

gdzie:
- $ L $ jest całkowitą funkcją kosztu.
- $ y_i $ są rzeczywistymi wartościami zmiennej zależnej.
- $ \boldsymbol{x}_i^{\top} $ to transponowany wektor zmiennych niezależnych (cechy).
- $ \boldsymbol{\beta} $ to wektor współczynników modelu.
- $ \lambda $ to parametr regularyzacji, który kontroluje siłę kary stosowanej do współczynników.

### Główne cechy regresji grzbietowej
Regresja grzbietowa, znana również jako regresja L2, wprowadza człon regularyzacyjny, który wymusza na modelu utrzymanie jak najmniejszych wartości wag. Ten człon, opisany jako $ \lambda \sum_{j=1}^{n} \beta_j^2 $, przeciwdziała nadmiernemu dopasowaniu poprzez zmniejszenie współczynników w kierunku zera, ale nie ustawia ich dokładnie na zero. Dzięki temu wszystkie cechy pozostają w modelu.

Regresja grzbietowa jest szczególnie skuteczna w sytuacjach, gdy zmienne predykcyjne są silnie skorelowane. Podobnie jak w przypadku modelu LASSO, regularyzacja zmniejsza wariancję i zapewnia bardziej niezawodne oszacowania, co jest kluczowe w obliczeniach statystycznych.

### Zalety regresji grzbietowej

Jedną z głównych zalet regresji grzbietowej jest stabilność oszacowań. W porównaniu do zwykłej regresji najmniejszych kwadratów, regresja grzbietowa dostarcza bardziej stabilnych wyników, szczególnie w przestrzeniach wielowymiarowych, gdzie występuje współliniowość między zmiennymi. 

Dodatkowo, metoda ta skutecznie zarządza kompromisem między odchyleniem a wariancją. Wprowadzając karę za rozmiar współczynników, regresja grzbietowa pomaga osiągnąć lepszą generalizację na niewidzianych danych, co jest istotne dla praktycznych zastosowań modelowania.

### Ograniczenia regresji grzbietowej

W przeciwieństwie do regresji Lasso, nie wykonuje wyboru zmiennej; wszystkie predyktory pozostają w modelu. To może być nieidealne z punktu widzenia łatwości interpretacji, zwłaszcza w przypadkach z wieloma cechami.

Kolejnym ograniczeniem jest wrażliwość na wybór parametru regularyzacji $ \lambda $. Wartość zbyt wysoka może prowadzić do niedopasowania modelu do danych, podczas gdy zbyt niska wartość może nie wystarczająco rozwiązać problem nadmiernego dopasowania.
